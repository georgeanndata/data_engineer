{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc, jsparkSession=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Chapters\n",
    "bookChaptersDF = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(\"bookcontents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----+\n",
      "|Chapter|                Name|Page|\n",
      "+-------+--------------------+----+\n",
      "|      1|        Introduction|  11|\n",
      "|      2|Basic Engineering...|  19|\n",
      "|      3|Advanced Engineer...|  28|\n",
      "|      4|     Hands On Course|  60|\n",
      "|      5|        Case Studies|  62|\n",
      "|      6|Best Practices Cl...|  73|\n",
      "|      7|130+ Data Sources...|  77|\n",
      "|      8|1001 Interview Qu...|  82|\n",
      "|      9|Recommended Books...|  87|\n",
      "+-------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bookChaptersDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from DataFrame\n",
    "bookRDD = bookChaptersDF.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Chapter=1, Name='Introduction', Page=11)\n",
      "Row(Chapter=2, Name='Basic Engineering Skills', Page=19)\n",
      "Row(Chapter=3, Name='Advanced Engineering Skills', Page=28)\n",
      "Row(Chapter=4, Name='Hands On Course', Page=60)\n",
      "Row(Chapter=5, Name='Case Studies', Page=62)\n"
     ]
    }
   ],
   "source": [
    "# Inspect RDD\n",
    "for row in bookRDD.take(5): \n",
    "        print(row) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify RDD to create compound column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compound column\n",
    "splitRDD = bookRDD.map(lambda columns: (columns[0],(str(columns[2]) + \"/\" + columns[1] )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, '11/Introduction')\n",
      "(2, '19/Basic Engineering Skills')\n",
      "(3, '28/Advanced Engineering Skills')\n",
      "(4, '60/Hands On Course')\n",
      "(5, '62/Case Studies')\n"
     ]
    }
   ],
   "source": [
    "# Inspect new RDD\n",
    "for row in splitRDD.take(5):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn RDD back to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create schema for DataFrame\n",
    "from pyspark.sql.types import *\n",
    "compoundSchema = StructType([\n",
    "StructField(\"Chapter\",IntegerType()),\n",
    "StructField(\"Compound\",StringType()),\n",
    "#StructField(\"Page\",StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "compoundDF = spark.createDataFrame(splitRDD,compoundSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|Chapter|            Compound|\n",
      "+-------+--------------------+\n",
      "|      1|     11/Introduction|\n",
      "|      2|19/Basic Engineer...|\n",
      "|      3|28/Advanced Engin...|\n",
      "|      4|  60/Hands On Course|\n",
      "|      5|     62/Case Studies|\n",
      "|      6|73/Best Practices...|\n",
      "|      7|77/130+ Data Sour...|\n",
      "|      8|82/1001 Interview...|\n",
      "|      9|87/Recommended Bo...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compoundDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file into RDD\n",
    "sectionsRDD = sc.textFile(\"sections_wordcount.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1.1,What is this Cookbook\n",
      "1,1.2,Data Engineer vs Data Scientist\n",
      "1,1.3,My Data Science Platform Blueprint\n",
      "1,1.4,Who Companies Need\n",
      "2,2.1,Learn To Code\n"
     ]
    }
   ],
   "source": [
    "# Inspect new RDD\n",
    "for row in sectionsRDD.take(5):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each row\n",
    "playRDD = sectionsRDD.map(lambda columns: columns.split(\",\"))\n",
    "                              #(columns[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1.1', 'What is this Cookbook']\n",
      "['1', '1.2', 'Data Engineer vs Data Scientist']\n",
      "['1', '1.3', 'My Data Science Platform Blueprint']\n",
      "['1', '1.4', 'Who Companies Need']\n",
      "['2', '2.1', 'Learn To Code']\n"
     ]
    }
   ],
   "source": [
    "# Inspect new RDD\n",
    "for row in playRDD.take(5):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take 3rd column (text)\n",
    "selecttextRDD = playRDD.map(lambda columns: columns[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is this Cookbook\n",
      "Data Engineer vs Data Scientist\n",
      "My Data Science Platform Blueprint\n",
      "Who Companies Need\n",
      "Learn To Code\n"
     ]
    }
   ],
   "source": [
    "# Inspect new RDD\n",
    "for row in selecttextRDD.take(5):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten RDD\n",
    "flatRDD = selecttextRDD.flatMap(lambda text: text.split(\" \")).map(lambda word: (word,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('What', 1)\n",
      "('is', 1)\n",
      "('this', 1)\n",
      "('Cookbook', 1)\n",
      "('Data', 1)\n"
     ]
    }
   ],
   "source": [
    "# Inspect new RDD\n",
    "for row in flatRDD.take(5):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the Words and sort by key\n",
    "reducedRDD = flatRDD.reduceByKey(lambda v1,v2: v1+v2).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('(AWS)', 1)\n",
      "('(GCP)', 1)\n",
      "('A', 2)\n",
      "('API', 1)\n",
      "('About', 1)\n",
      "('Academic', 1)\n",
      "('Agile', 1)\n",
      "('Airbnb', 1)\n",
      "('Amazon', 2)\n",
      "('And', 6)\n",
      "('Apache', 3)\n",
      "('Articles', 1)\n",
      "('Azure', 1)\n",
      "('BMW', 1)\n",
      "('Baidu', 1)\n",
      "('Blackrock', 1)\n",
      "('Blog', 1)\n",
      "('Blueprint', 1)\n",
      "('Booking.com', 1)\n",
      "('Books', 2)\n"
     ]
    }
   ],
   "source": [
    "# Inspect new RDD\n",
    "for row in reducedRDD.take(20):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn RDD back into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create schema for Wordcount DataFrame\n",
    "from pyspark.sql.types import *\n",
    "wordcountSchema = StructType([\n",
    "StructField(\"Word\",StringType()),\n",
    "StructField(\"Count\",IntegerType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       Word|Count|\n",
      "+-----------+-----+\n",
      "|      (AWS)|    1|\n",
      "|      (GCP)|    1|\n",
      "|          A|    2|\n",
      "|        API|    1|\n",
      "|      About|    1|\n",
      "|   Academic|    1|\n",
      "|      Agile|    1|\n",
      "|     Airbnb|    1|\n",
      "|     Amazon|    2|\n",
      "|        And|    6|\n",
      "|     Apache|    3|\n",
      "|   Articles|    1|\n",
      "|      Azure|    1|\n",
      "|        BMW|    1|\n",
      "|      Baidu|    1|\n",
      "|  Blackrock|    1|\n",
      "|       Blog|    1|\n",
      "|  Blueprint|    1|\n",
      "|Booking.com|    1|\n",
      "|      Books|    2|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "wordcountDF = spark.createDataFrame(reducedRDD,wordcountSchema)\n",
    "wordcountDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       Word|Count|\n",
      "+-----------+-----+\n",
      "|       Data|   48|\n",
      "|    Science|   40|\n",
      "|        And|    6|\n",
      "|   Platform|    3|\n",
      "| Processing|    3|\n",
      "|     Apache|    3|\n",
      "|        and|    3|\n",
      "|       What|    2|\n",
      "|     Amazon|    2|\n",
      "|      Books|    2|\n",
      "|    Courses|    2|\n",
      "|         to|    2|\n",
      "|       Nifi|    2|\n",
      "|    Twitter|    2|\n",
      "|   Security|    2|\n",
      "|         To|    2|\n",
      "|      Learn|    2|\n",
      "|      Cloud|    2|\n",
      "|          A|    2|\n",
      "|Development|    2|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame after Count column\n",
    "wordcountDF.sort(wordcountDF.Count.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
